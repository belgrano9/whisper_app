{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "*   Add functionality to treat big files. Split them into chunks-\n",
    "*   Start trials with live speech functionality trasncription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tomso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\datasets\\load.py:1454: FutureWarning: The repository for hf-internal-testing/librispeech_asr_dummy contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hf-internal-testing/librispeech_asr_dummy\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f0de36b1ae48d4a5728d1c25974172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f022cdcc15f14873ae379679a7c31eb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "395e77bc9a364a1bbc0b2b32bec2c52f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fdd90055ed5495694789f5a0d9f6fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/805 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7bf2dcf6f114f0291bc4666bbfcc13f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac95cb0f87d34fbf98ed7bfeeb452fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.41M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dc33d27981547ddbdcfe998ed743db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2698acce9f7140e8849d0cb8fee145a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d6fd7d78a348f297df39a90ffb6f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc8823ac1ba04fd3a1b4d848baee73ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.83k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f393433ba7040ea8436cf0100dfb3a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\tomso\\AppData\\Roaming\\Python\\Python39\\site-packages\\gradio\\queueing.py\", line 495, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"C:\\Users\\tomso\\AppData\\Roaming\\Python\\Python39\\site-packages\\gradio\\route_utils.py\", line 232, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"C:\\Users\\tomso\\AppData\\Roaming\\Python\\Python39\\site-packages\\gradio\\blocks.py\", line 1561, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"C:\\Users\\tomso\\AppData\\Roaming\\Python\\Python39\\site-packages\\gradio\\blocks.py\", line 1179, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"c:\\Users\\tomso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\anyio\\to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"c:\\Users\\tomso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"c:\\Users\\tomso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\tomso\\AppData\\Roaming\\Python\\Python39\\site-packages\\gradio\\utils.py\", line 695, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"<ipython-input-2-66e659bf5b1d>\", line 45, in transcribe_nonlive\n",
      "    return transcriber({\"sampling_rate\": sr, \"raw\": y})[\"text\"]\n",
      "  File \"c:\\Users\\tomso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\pipelines\\automatic_speech_recognition.py\", line 357, in __call__\n",
      "    return super().__call__(inputs, **kwargs)\n",
      "  File \"c:\\Users\\tomso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 1132, in __call__\n",
      "    return next(\n",
      "  File \"c:\\Users\\tomso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py\", line 124, in __next__\n",
      "    item = next(self.iterator)\n",
      "  File \"c:\\Users\\tomso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py\", line 266, in __next__\n",
      "    processed = self.infer(next(self.iterator), **self.params)\n",
      "  File \"c:\\Users\\tomso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 630, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"c:\\Users\\tomso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 674, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"c:\\Users\\tomso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 32, in fetch\n",
      "    data.append(next(self.dataset_iter))\n",
      "  File \"c:\\Users\\tomso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py\", line 183, in __next__\n",
      "    processed = next(self.subiterator)\n",
      "  File \"c:\\Users\\tomso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\pipelines\\automatic_speech_recognition.py\", line 466, in preprocess\n",
      "    inputs = F.resample(\n",
      "  File \"c:\\Users\\tomso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchaudio\\functional\\functional.py\", line 1528, in resample\n",
      "    resampled = _apply_sinc_resample_kernel(waveform, orig_freq, new_freq, gcd, kernel, width)\n",
      "  File \"c:\\Users\\tomso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchaudio\\functional\\functional.py\", line 1463, in _apply_sinc_resample_kernel\n",
      "    waveform = torch.nn.functional.pad(waveform, (width, width + orig_freq))\n",
      "RuntimeError: [enforce fail at alloc_cpu.cpp:80] data. DefaultCPUAllocator: not enough memory: you tried to allocate 286199267328 bytes.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\tomso\\AppData\\Roaming\\Python\\Python39\\site-packages\\gradio\\queueing.py\", line 495, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"C:\\Users\\tomso\\AppData\\Roaming\\Python\\Python39\\site-packages\\gradio\\route_utils.py\", line 232, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"C:\\Users\\tomso\\AppData\\Roaming\\Python\\Python39\\site-packages\\gradio\\blocks.py\", line 1561, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"C:\\Users\\tomso\\AppData\\Roaming\\Python\\Python39\\site-packages\\gradio\\blocks.py\", line 1179, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"c:\\Users\\tomso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\anyio\\to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"c:\\Users\\tomso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"c:\\Users\\tomso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\tomso\\AppData\\Roaming\\Python\\Python39\\site-packages\\gradio\\utils.py\", line 695, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"<ipython-input-2-66e659bf5b1d>\", line 45, in transcribe_nonlive\n",
      "    return transcriber({\"sampling_rate\": sr, \"raw\": y})[\"text\"]\n",
      "  File \"c:\\Users\\tomso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\pipelines\\automatic_speech_recognition.py\", line 357, in __call__\n",
      "    return super().__call__(inputs, **kwargs)\n",
      "  File \"c:\\Users\\tomso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 1132, in __call__\n",
      "    return next(\n",
      "  File \"c:\\Users\\tomso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py\", line 124, in __next__\n",
      "    item = next(self.iterator)\n",
      "  File \"c:\\Users\\tomso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py\", line 266, in __next__\n",
      "    processed = self.infer(next(self.iterator), **self.params)\n",
      "  File \"c:\\Users\\tomso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 630, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"c:\\Users\\tomso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 674, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"c:\\Users\\tomso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 32, in fetch\n",
      "    data.append(next(self.dataset_iter))\n",
      "  File \"c:\\Users\\tomso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py\", line 183, in __next__\n",
      "    processed = next(self.subiterator)\n",
      "  File \"c:\\Users\\tomso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\pipelines\\automatic_speech_recognition.py\", line 466, in preprocess\n",
      "    inputs = F.resample(\n",
      "  File \"c:\\Users\\tomso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchaudio\\functional\\functional.py\", line 1528, in resample\n",
      "    resampled = _apply_sinc_resample_kernel(waveform, orig_freq, new_freq, gcd, kernel, width)\n",
      "  File \"c:\\Users\\tomso\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchaudio\\functional\\functional.py\", line 1463, in _apply_sinc_resample_kernel\n",
      "    waveform = torch.nn.functional.pad(waveform, (width, width + orig_freq))\n",
      "RuntimeError: [enforce fail at alloc_cpu.cpp:80] data. DefaultCPUAllocator: not enough memory: you tried to allocate 54702024192 bytes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "models = [\"openai/whisper-tiny.en\",\"openai/whisper-small.en\", \"openai/whisper-medium.en\"]\n",
    "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n",
    "#load samples\n",
    "def prepare_audio_samples(ds, num_samples=5):\n",
    "  \"\"\"\n",
    "  Look for ``num_samples`` in the ``ds`` given.\n",
    "  \"\"\"\n",
    "  return [[ds[i]['file']] for i in range(num_samples)]  # Adjust index range as needed\n",
    "\n",
    "def pipe(model, chunk_lenght=30):\n",
    "  \"\"\"\n",
    "  Note about chunking from HF: The Whisper model is intrinsically designed to work on audio samples of up to \n",
    "  30s in duration. However, by using a chunking algorithm, it can be used to \n",
    "  transcribe audio samples of up to arbitrary length. This is possible through \n",
    "  Transformers pipeline method. Chunking is enabled by setting chunk_length_s=30\n",
    "   when instantiating the pipeline.\n",
    "  \"\"\"\n",
    "  return pipeline(\"automatic-speech-recognition\", model=model, chunk_length_s=chunk_lenght)\n",
    "\n",
    "def transcribe_live(stream, new_chunk, model):\n",
    "    sr, y = new_chunk\n",
    "    y = y.astype(np.float32)\n",
    "    y /= np.max(np.abs(y))\n",
    "\n",
    "    if stream is not None:\n",
    "        stream = np.concatenate([stream, y])\n",
    "    else:\n",
    "        stream = y\n",
    "    transcriber = pipe(model, 90)\n",
    "    return stream, transcriber({\"sampling_rate\": sr, \"raw\": stream})[\"text\"]\n",
    "\n",
    "def transcribe_nonlive(audio, model):\n",
    "    sr, y = audio\n",
    "    y = y.astype(np.float32)\n",
    "    y /= np.max(np.abs(y))\n",
    "\n",
    "    transcriber = pipe(model, 90)\n",
    "    return transcriber({\"sampling_rate\": sr, \"raw\": y})[\"text\"]\n",
    "\n",
    "title = \"\"\"\n",
    "# Welcome to my ASR Demo with Gradio\n",
    "\n",
    "Here you can test the Speech to Text Models performance both live and with recorded files.\n",
    "Have fun!\n",
    "\"\"\"\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(title)\n",
    "    with gr.Tab(\"Live Recording\"):\n",
    "        gr.Interface(\n",
    "            fn=transcribe_live,\n",
    "            inputs=[\"state\", gr.Audio(sources=[\"microphone\"], streaming=True), gr.Dropdown(choices=models, label=\"Model\")],\n",
    "            outputs=[\"state\", \"text\"],\n",
    "            live=True,\n",
    "        )\n",
    "    with gr.Tab(\"Simple transcription\"):\n",
    "        gr.Interface(\n",
    "            fn=transcribe_nonlive,\n",
    "            inputs=[gr.Audio(), gr.Dropdown(choices=models, label=\"Model\")],\n",
    "            outputs=\"text\",\n",
    "            examples=prepare_audio_samples(ds)\n",
    "        )\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
